{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7683dbc3",
   "metadata": {},
   "source": [
    "## ðŸ“Œ EDA ì˜ˆìƒ ë¬¸ì œ 1\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **â€œì¤‘ê°„ê³ ì‚¬â€, â€œê¸°ë§ê³ ì‚¬â€** ì»¬ëŸ¼ì´ **90 ì´ìƒ**ì¸ ì»¬ëŸ¼ì„ ë³€ìˆ˜ **column**ì— ì €ìž¥ í›„ í•´ë‹¹ ì»¬ëŸ¼ë§Œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "1. **ì‚°ì ë„ ê·¸ëž˜í”„**ë¥¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. â€œ**ì¶œê²°ìƒí™©**â€ ì»¬ëŸ¼ì„ **x**ì¶•ì—, â€œ**ìˆ˜í–‰í‰ê°€**â€ ì»¬ëŸ¼ì„ **y**ì— í• ë‹¹í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_top_student(df):\n",
    "    column = (df['ì¤‘ê°„ê³ ì‚¬'] >= 90) & (df['ê¸°ë§ê³ ì‚¬'] >=90)\n",
    "    top_student = df[column]\n",
    "\n",
    "    return top_student\n",
    "\n",
    "def make_scatter(df):\n",
    "    x = df['ì¶œê²°ìƒí™©']\n",
    "    y = df['ìˆ˜í–‰í‰ê°€']\n",
    "\n",
    "    student_plot = plt.scatter(x,y)\n",
    "    plt.show()\n",
    "    return student_plot\n",
    "\n",
    "def main():\n",
    "    df_student = pd.read_csv('./data/student.csv')\n",
    "    df_top_student = get_top_student(df_student)\n",
    "    plot_scatter = make_scatter(df_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf528d6",
   "metadata": {},
   "source": [
    "## ðŸ“Œ EDA ì˜ˆìƒ ë¬¸ì œ 2\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **`matplotlib`**ì˜ **`pyplot`**ë¥¼ **import** í•˜ê³  **plt** ë³„ì¹­ì„ ë¶™ì´ì„¸ìš”.\n",
    "\n",
    "1. **df** ì—ì„œ **ê°€ìž¥ ìž‘ì€** **â€œí•™ë…„â€** ì»¬ëŸ¼ë“¤ì˜ **â€œê¸°ë§ê³ ì‚¬â€** ì ìˆ˜ì˜ **í‰ê· **ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. **ì†Œìˆ˜ì  ë„·ì§¸ ìžë¦¬**ì—ì„œ ë°˜ì˜¬ë¦¼ í›„ ë°˜í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. **df** ì—ì„œ **â€œì¤‘ê°„ê³ ì‚¬â€**ì™€ **â€œê¸°ë§ê³ ì‚¬â€**ì˜ **ìƒê´€ê³„ìˆ˜**ë¥¼ êµ¬í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. **ì†Œìˆ˜ì  ë„·ì§¸ ìžë¦¬**ì—ì„œ ë°˜ì˜¬ë¦¼ í›„ ë°˜í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. **df**ì—ì„œ **â€œê¸°ë§ê³ ì‚¬â€**ì˜ ë¶„í¬ë¥¼ **ížˆìŠ¤í† ê·¸ëž¨**ìœ¼ë¡œ **ì‹œê°í™”**í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. **ê°€ë¡œ 10, ì„¸ë¡œ 5**ë¡œ ì‹œê°í™”í•˜ì„¸ìš”.\n",
    "    2. **ë§‰ëŒ€ì˜ ê°œìˆ˜**ëŠ” **10**ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "    3. **plt**ë¥¼ ì´ìš©í•´ **ê·¸ëž˜í”„**ë¥¼ ê·¸ë¦¬ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def youngest_average_mean(df):\n",
    "    youngest = df['í•™ë…„'].min()\n",
    "    return round(df[df['í•™ë…„'] == youngest]['ê¸°ë§ê³ ì‚¬'].mean(), 3)\n",
    "\n",
    "def midterm_final_corr(df):\n",
    "    return round(df['ì¤‘ê°„ê³ ì‚¬'].corr(df['ê¸°ë§ê³ ì‚¬']),3)\n",
    "def plot_final_histogram(df):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(df['ê¸°ë§ê³ ì‚¬'], bins=10)\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/student.csv')\n",
    "    one = youngest_average_mean(df)\n",
    "    two = midterm_final_corr(df)\n",
    "    three = plot_final_histogram(df)\n",
    "\n",
    "    return one, two, three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55c0c3",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ EDA ì˜ˆìƒ ë¬¸ì œ 3\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **â€œì¡¸ì—…ì—¬ë¶€â€ ì»¬ëŸ¼ì´ 1ì¸ ë°ì´í„°ì˜ ê°œìˆ˜**ë¥¼ ì„¸ì–´ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    - í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í–‰ì˜ ìˆ˜ë¥¼ êµ¬í•œ ë’¤, **ì •ìˆ˜í˜•ìœ¼ë¡œ ë°˜í™˜**í•˜ì„¸ìš”.\n",
    "\n",
    "1. **ê²°ì¸¡ì¹˜(NaN)ë¥¼ ì œê±°**í•œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    - dropna()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì œê±°í•˜ê³  **ìƒˆë¡œìš´ DataFrame**ì„ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "1. **â€œê¸°ë§ê³ ì‚¬â€ ì»¬ëŸ¼ì˜ ë°•ìŠ¤ í”Œë¡¯**ì„ ê·¸ë¦¬ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    - plt.boxplot()ì„ ì‚¬ìš©í•˜ê³ , show_plot(\"box\") í•¨ìˆ˜ë¥¼ ì´ìš©í•´ **ê·¸ëž˜í”„ë¥¼ ì €ìž¥í•˜ê³  ë°˜í™˜**í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "DATA_PATH = './data/student.csv'\n",
    "\n",
    "def show_plot(fig_name):\n",
    "    plt.savefig(fig_name + \".png\")\n",
    "    return Image.open(fig_name + \".png\")\n",
    "\n",
    "def get_graduated_count(df):\n",
    "    return len(df[df['ì¡¸ì—…ì—¬ë¶€'] == 1])\n",
    "def remove_nan(df):\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "def make_box(df):\n",
    "    plt.boxplot(df['ê¸°ë§ê³ ì‚¬'])\n",
    "    box = show_plot(\"box\")\n",
    "    return box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293532c",
   "metadata": {},
   "source": [
    "## ðŸ“Œ íšŒê·€ ì˜ˆìƒ ë¬¸ì œ 4\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. `sklearn`ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `ensemble`ëª¨ë“ˆ ì•ˆì— `RandomForestRegressor`  import í•˜ì„¸ìš”.\n",
    "\n",
    "1. **ë…ë¦½ë³€ìˆ˜**ì™€ **ì¢…ì†ë³€ìˆ˜**ë¡œ ë‚˜ëˆ„ëŠ” **get_X_y** êµ¬í˜„í•˜ì„¸ìš”.\n",
    "    1. **â€œidâ€, â€œë°˜â€** ì»¬ëŸ¼ ì œì™¸í•˜ê³  **ë³€ìˆ˜ X** ì— ì €ìž¥\n",
    "    2. **â€œê¸°ë§ê³ ì‚¬â€** ì»¬ëŸ¼ì„ ì‹œë¦¬ì¦ˆ í˜•íƒœë¡œ **ë³€ìˆ˜ y** ì— ì €ìž¥\n",
    "\n",
    "1. ëª¨ë¸ ê°ì²´ë¥¼ ë³€ìˆ˜ **model**ì— í• ë‹¹í•˜ê³ , **x_train, y_train**ì„ ì´ìš©í•´ **í•™ìŠµ**ì‹œí‚¤ì„¸ìš”.\n",
    "    1. **`max_depth`**ëŠ” **3**ìœ¼ë¡œ, **`n_estimators`**ëŠ” **100**ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "\n",
    "1. í•™ìŠµì´ ì™„ë£Œëœ **model**ì„ ì‚¬ìš©í•´ **ì˜ˆì¸¡**í•˜ê³  ë³€ìˆ˜ **pred**ì— **ì‹œë¦¬ì¦ˆ í˜•íƒœ**ë¡œ ì €ìž¥í•˜ì„¸ìš”.\n",
    "    1. **df_test**ì—ì„œ **â€œidâ€** ì»¬ëŸ¼ì„ **ì œì™¸**í•œ ë°ì´í„°ë¡œ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_X_y(df):\n",
    "    X = df.drop(columns=['id','ë°˜'], axis=1)\n",
    "    y = df['ê¸°ë§ê³ ì‚¬']\n",
    "    return X, y\n",
    "def main():\n",
    "    df_train = pd.read_csv('./data/student.csv')\n",
    "    df_test = pd.read_csv('./data/submission.csv')\n",
    "\n",
    "    x_train, y_train = get_X_y(df_train)\n",
    "\n",
    "    model = RandomForestRegressor(max_depth=3, n_estimators=100)\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    pred = model.predict(df_test.drop(columns =['id'], axis=1))\n",
    "\n",
    "    return model, pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f0126",
   "metadata": {},
   "source": [
    "## ðŸ“Œ ì„ í˜• íšŒê·€ & ë¦¿ì§€ íšŒê·€ ì˜ˆìƒ ë¬¸ì œ 5\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. `LinearRegression`, `Ridge` ëª¨ë¸ì„ import í•˜ì„¸ìš”.\n",
    "\n",
    "1. **â€œê¸°ë§ê³ ì‚¬â€** ì»¬ëŸ¼ì„ **ì¢…ì† ë³€ìˆ˜**ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ **ë…ë¦½ ë³€ìˆ˜**ë¡œ ë¶„ë¦¬í•˜ëŠ” `get_X_y()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "    1. **X**ì—ëŠ” **â€œê¸°ë§ê³ ì‚¬â€ ì œì™¸**í•œ ë‚˜ë¨¸ì§€ ì»¬ëŸ¼, **y**ì—ëŠ” **â€œê¸°ë§ê³ ì‚¬â€ ì»¬ëŸ¼ë§Œ** ì €ìž¥í•˜ì„¸ìš”.\n",
    "\n",
    "1. ì„ í˜• íšŒê·€, ë¦¿ì§€ íšŒê·€ ëª¨ë¸ì„ ì„ ì–¸í•˜ê³  í•™ìŠµì‹œí‚¤ì„¸ìš”.\n",
    "    1. ë¦¿ì§€ íšŒê·€ëŠ” `alpha=0.3` ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "\n",
    "1. í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ ê°ê° `lr_pred`, `ridge_pred`ì— ì €ìž¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "def get_X_y(df):\n",
    "    X = df.drop(columns=['ê¸°ë§ê³ ì‚¬'], axis=1)\n",
    "    y = df['ê¸°ë§ê³ ì‚¬']\n",
    "    return X,y\n",
    "\n",
    "TRAIN_DATA = \"./data/student_train.csv\"\n",
    "TEST_DATA = \"./data/student_test.csv\"\n",
    "\n",
    "def main():\n",
    "    df_train = pd.read_csv(TRAIN_DATA)\n",
    "    df_test = pd.read_csv(TEST_DATA)\n",
    "\n",
    "    x_train, y_train = get_X_y(df_train)\n",
    "\n",
    "    linear_model = LinearRegression()\n",
    "    ridge_model = Ridge(alpha=0.3)\n",
    "\n",
    "    linear_model.fit(x_train,y_train)\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    linear_pred = linear_model.predict(df_test)\n",
    "    ridge_model = ridge_model.predict(df_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101f4b0",
   "metadata": {},
   "source": [
    "## ðŸ“Œ SVM ì˜ˆìƒ ë¬¸ì œ 6\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(`MinMaxScaler`, `SVC`)ë¥¼ import í•˜ì„¸ìš”.\n",
    "\n",
    "1. `index`, **â€œì¡¸ì—…ì—¬ë¶€â€**ë¥¼ ì œì™¸í•œ ë³€ìˆ˜ë“¤ì„ ë…ë¦½ë³€ìˆ˜ë¡œ, **â€œì¡¸ì—…ì—¬ë¶€â€** ë¥¼ ì¢…ì†ë³€ìˆ˜ë¡œ ë¶„ë¦¬í•˜ëŠ” `extract_features_labels()` í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œëŠ” `index`ë§Œ ì œì™¸í•˜ê³  ë…ë¦½ë³€ìˆ˜ë§Œ ì¶”ì¶œí•˜ëŠ” `extract_features()` í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "1. `MinMaxScaler`ë¡œ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ **ì •ê·œí™”**í•˜ì„¸ìš”.\n",
    "\n",
    "1. `SVC` ëª¨ë¸ì„ **í•™ìŠµ**ì‹œí‚¤ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ **ì˜ˆì¸¡**ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "1. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ **â€œì¡¸ì—…ì—¬ë¶€â€**ì»¬ëŸ¼ì— ì¶”ê°€í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MixMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def extract_features_labels(df):\n",
    "    features = df.drop(columns=['index', 'ì¡¸ì—…ì—¬ë¶€'], axis=1)\n",
    "    labels = df['ì¡¸ì—…ì—¬ë¶€']\n",
    "    return features , labels\n",
    "\n",
    "def extract_features(df):\n",
    "    features = df.drop(columns=['index'], axis=1)\n",
    "    return features\n",
    "\n",
    "def scale_features(train_features, test_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_features)\n",
    "    test_scaled = scaler.transform(test_features)\n",
    "    return train_scaled, test_scaled\n",
    "\n",
    "TRAIN_PATH = \"./data/student_train.csv\"\n",
    "TEST_PATH = \"./data/student_test.csv\"\n",
    "\n",
    "def main():\n",
    "    train_df = load_data(TRAIN_PATH)\n",
    "    test_df = load_data(TEST_PATH)\n",
    "\n",
    "    X_train, y_train = extract_features_labels(train_df)\n",
    "    X_test = extract_features(test_df)\n",
    "    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
    "\n",
    "    svc_model = SVC()\n",
    "    svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    pred = svc_model.predict(X_test_scaled)\n",
    "    test_df['ì¡¸ì—…ì—¬ë¶€'] = pred\n",
    "    return svc_model, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dcb489",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ í…ì„œ ì—°ì‚° ì˜ˆìƒ ë¬¸ì œ 7\n",
    "\n",
    "### ðŸ“Â **ì§€ì‹œì‚¬í•­**\n",
    "\n",
    "1. **í…ì„œí”Œë¡œìš°**ì˜ aì™€ bë¥¼ ë”í•´ â€˜**add**â€™ì— ì €ìž¥í•˜ì„¸ìš”.\n",
    "\n",
    "1. **í…ì„œí”Œë¡œìš°**ì˜ aì™€ bë¥¼ ë¹¼ â€˜**sub**â€™ì— ì €ìž¥í•˜ì„¸ìš”.\n",
    "\n",
    "1. **í…ì„œí”Œë¡œìš°**ì˜ aì™€ bë¥¼ ê³±í•´ â€˜**mul**â€™ì— ì €ìž¥í•˜ì„¸ìš”.\n",
    "\n",
    "1. **í…ì„œí”Œë¡œìš°**ì˜ aì™€ bë¥¼ ë‚˜ëˆ  â€˜**div**â€™ì— ì €ìž¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "\n",
    "    a = tf.constant(10, dtype = tf.int32)\n",
    "    b = tf.constant(3, dtype = tf.int32)\n",
    "\n",
    "    add = tf.add(a,b)\n",
    "    sub = tf.subtract(a,b)\n",
    "    mul = tf.multiply(a,b)\n",
    "    div = tf.truediv(a,b)\n",
    "\n",
    "    tf.add(tf.multiply(a,b),c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4b216",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ ë°ì´í„° ì „ì²˜ë¦¬ ì˜ˆìƒ ë¬¸ì œ 8\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **path** ë¥¼ ìž…ë ¥ë°›ì•„ **csv** íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.\n",
    "\n",
    "1. **ë²”ì£¼í˜• ë°ì´í„°**ì¸ ì»¬ëŸ¼ â€œ**status**â€ ì„ **ìˆ«ìží˜• ë°ì´í„°**ë¡œ ë³€í™˜ â†’ **ì‹œë¦¬ì¦ˆ í˜•íƒœ**ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "    1. **â€˜Lowâ€™ : 3**\n",
    "    2. **â€˜Middleâ€™ : 2**\n",
    "    3. **â€˜Highâ€™ : 1**\n",
    "\n",
    "1. **í•™ìŠµìš© ë°ì´í„°**ì™€ **ê²€ì¦ìš© ë°ì´í„°**ë¥¼ ë‚˜ëˆ  **ë…ë¦½ë³€ìˆ˜**, **ì¢…ì†ë³€ìˆ˜**ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "    1. **ì¢…ì†ë³€ìˆ˜** â€œ**y**â€ = â€˜**price**â€™ ì»¬ëŸ¼\n",
    "    2. **ë…ë¦½ë³€ìˆ˜** â€œ**X**â€ = â€˜**price**â€™ ì»¬ëŸ¼ì„ **ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì»¬ëŸ¼** ì „ì²´\n",
    "    3. **í•™ìŠµìš©, ê²€ì¦ìš©** ë°ì´í„° ë¹„ìœ¨ì€ **6 : 4** ìž…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96303e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_PATH = \"data/train.csv\"\n",
    "\n",
    "def load_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def label_encoding(series):\n",
    "    status_map = {\n",
    "        'Low':3,\n",
    "        'Middle':2,\n",
    "        'High':1\n",
    "    }\n",
    "    return series.map(status_map)\n",
    "\n",
    "def divide_data(df):\n",
    "    features = df.drop(columns=['price'], axis=1)\n",
    "    X = df[features]\n",
    "    y = df['price']\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.4)\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def main():\n",
    "    train = load_csv(TRAIN_PATH)\n",
    "    train['status'] = label_encoding(train['status'])\n",
    "    X_train, X_valid, y_train, y_valid = divide_data(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386d441",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ ë°ì´í„° ì „ì²˜ë¦¬ ì˜ˆìƒ ë¬¸ì œ 9\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. `fill_missing_with_mean(df, column)` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì—¬,\n",
    "    - `'ì˜ì–´ì ìˆ˜'` ì»¬ëŸ¼ì— ìžˆëŠ” **ê²°ì¸¡ê°’(NaN)** ì„ í•´ë‹¹ ì»¬ëŸ¼ì˜ **í‰ê· ê°’**ìœ¼ë¡œ ì±„ì›Œë„£ìœ¼ì„¸ìš”.\n",
    "\n",
    "1. `calculate_iqr_bounds(df, column)` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì—¬,\n",
    "    - `'ì˜ì–´ì ìˆ˜'` ì»¬ëŸ¼ì— ëŒ€í•´ **IQR ë°©ì‹ì„ ì´ìš©í•œ ì´ìƒì¹˜ ê²½ê³„ê°’**ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "    - ì•„ëž˜ ê³µì‹ì„ ë”°ë¥´ì„¸ìš”:\n",
    "        - Q1 = 1ì‚¬ë¶„ìœ„ìˆ˜ (25%)\n",
    "        - Q3 = 3ì‚¬ë¶„ìœ„ìˆ˜ (75%)\n",
    "        - IQR = Q3 - Q1\n",
    "        - í•˜í•œê°’ = Q1 - 1.2 Ã— IQR\n",
    "        - ìƒí•œê°’ = Q3 + 1.2 Ã— IQR\n",
    "        - í•˜í•œê°’ê³¼ ìƒí•œê°’ ì‚¬ì´ì— ìžˆëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. `remove_outliers(df, column, lower, upper)` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì—¬,\n",
    "    - columnì˜ ê°’ì˜ ê°œìˆ˜ë¥¼ ë³€ìˆ˜ countsì— ì €ìž¥í•˜ì„¸ìš”.\n",
    "    - ë¼ë²¨ ì´ë¦„ì„ countì˜ indexë¡œ ì§€ì •í•˜ì„¸ìš”.\n",
    "    - autopctëŠ” â€œ`%1.1f%%`â€ë¡œ ì„¤ì •í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_mean(df, column):\n",
    "    mean_score = df[column].mean()\n",
    "    df[column] = df[column].fillna(mean_score)\n",
    "    return df\n",
    "\n",
    "def remove_outliers_iqr(df,coluumn):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    lower_bound = q1 - 1.5*iqr\n",
    "    upper_bound = q3 + 1.5*iqr\n",
    "    return df[(df[column]>=lower_bound) & df[column]<=upper_bound]\n",
    "\n",
    "def draw_pie_chart(df, column):\n",
    "    column = df[column].value_counts()\n",
    "    plt.pie(counts, labels=counts.index, autopct='%1.1f%%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155b081",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ STOME ì˜ˆìƒ ë¬¸ì œ 10\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **SMOTE** ê°ì²´ë¥¼ ë§Œë“¤ê³  ë°ì´í„°ë¥¼ **ì˜¤ë²„ìƒ˜í”Œë§** í•˜ì„¸ìš”.\n",
    "    1. **imblearn** íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•´ **SMOTE** ê°ì²´ë¥¼ ë§Œë“œì„¸ìš”. ì´ë•Œ **random_state**ëŠ” **32**ë¡œ ì§€ì •í•˜ì„¸ìš”.\n",
    "    2. **X**ì™€ **y ë§¤ê°œë³€ìˆ˜**ë¥¼ **X_smote**ì™€ **y_smote**ì— ì €ìž¥í•˜ì„¸ìš”.\n",
    "\n",
    "1. **RandomUnderSampler** ê°ì²´ë¥¼ ë§Œë“¤ê³  ë°ì´í„°ë¥¼ **ì–¸ë”ìƒ˜í”Œë§** í•˜ì„¸ìš”.\n",
    "    1. **random_state**ëŠ” **7**ë¡œ ì§€ì •í•˜ì„¸ìš”.\n",
    "    2. **X**ì™€ **y**ì— **ì–¸ë”ì‹¬í”Œë§**ì„ ì ìš©í•œ ë’¤ ë³€ìˆ˜ **X_rus**, **y_rus**ì— ì €ìž¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def preprocessing_data(df):\n",
    "    df = df.copy()\n",
    "    for col in df.coumns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            df[col] = LabelEncoder().fit_transform(df[col])\n",
    "        else :\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def smote(X,y):\n",
    "    smote = SMOTE(random_state=32)\n",
    "    X_smote, y_smote = smote.fit_resample(X,y)\n",
    "    return X_smote, y_smote\n",
    "\n",
    "def rus(X,y):\n",
    "    rus = RandomUnderSampler(random_state=7)\n",
    "    X_rus, y_rus = rus.fit_resample(X,y)\n",
    "    return X_rus, y_rus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431011c0",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ - íšŒê·€ ì˜ˆìƒ ë¬¸ì œ 11\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **combine_df()** í•¨ìˆ˜ì—ì„œ í•™ë…„ë³„ ì„±ì  ë°ì´í„°ë¥¼ df1 ì•„ëž˜ df2 ê²°í•©í•˜ì„¸ìš”.\n",
    "    - **ignore_index=True** ì˜µì…˜ ì‚¬ìš© = ì¸ë±ìŠ¤ ì´ˆê¸°\n",
    "\n",
    "1. **get_X_y()** í•¨ìˆ˜ì—ì„œ â€œ**ì¸¡ì •ì¼ìž**â€ë¥¼ ì œê±°í•˜ê³ , â€œ**ê¸°ë§ê³ ì‚¬**â€ë¥¼ **y**ë¡œ ì§€ì •í•˜ì„¸ìš”.\n",
    "    1. **X**ëŠ” **â€œê¸°ë§ê³ ì‚¬â€ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€**ë¡œ ì§€ì •í•˜ì„¸ìš”.\n",
    "\n",
    "1. **DecisionTreeRegressor**ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , **df_test**ì— ëŒ€í•´ **ì˜ˆì¸¡**í•˜ì„¸ìš”.\n",
    "    1. **`sklearn`**ì˜ **`tree`** ëª¨ë“ˆì—ì„œ **`DecisionTreeRegressor`** ë¥¼ import í•˜ì„¸ìš”. (ê¼­ ê¸°ì–µ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91408126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def combine_df(df1, df2):\n",
    "    df_combined = pd.concat([df1,df2], axis=0, ignore_index=True)\n",
    "    return df_combined\n",
    "\n",
    "def get_X_y(df):\n",
    "    df = df.drop(columns=['ì¸¡ì •ì¼ìž'], axis=1)\n",
    "    X = df.drop(columns=['ê¸°ë§ê³ ì‚¬'], axis=1)\n",
    "    y = df['ê¸°ë§ê³ ì‚¬']\n",
    "    return X,y\n",
    "\n",
    "def modeling(X, y , df_test):\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X,y)\n",
    "    pred = model.predict(df_test)\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95f9ee3",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ - ë¶„ë¥˜ ì˜ˆìƒ ë¬¸ì œ 12\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. `split_data(X, y)` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. ìž…ë ¥ê°’ `X`, íƒ€ê²Ÿê°’ `y`ë¥¼ ë°›ì•„ **í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬**í•˜ì„¸ìš”.\n",
    "    2. `train_test_split()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©°, `test_size=0.3`, `random_state=32`ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "    3. ë¶„ë¦¬ëœ 4ê°œì˜ ê°’ì„ ë°˜í™˜í•˜ì„¸ìš”: `X_train, X_test, y_train, y_test`\n",
    "\n",
    "1. `train_decision_tree(X_train, y_train)` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. `DecisionTreeClassifier`ë¥¼ ì‚¬ìš©í•´ í•™ìŠµ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ê³ , í•™ìŠµëœ ëª¨ë¸ì„ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "    2. ëª¨ë¸ì€ `random_state=32`ë¡œ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. `plot_feature_importances(model, feature_names)` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. í•™ìŠµëœ ëª¨ë¸ì˜ `feature_importances_` ì†ì„±ì„ ì‚¬ìš©í•´ **ê° íŠ¹ì„±ì˜ ì¤‘ìš”ë„**ë¥¼ ë§‰ëŒ€ê·¸ëž˜í”„ë¡œ ì‹œê°í™”í•˜ì„¸ìš”.\n",
    "    2. `plt.barh()` í•¨ìˆ˜ë¥¼ í™œìš©í•´ ìˆ˜í‰ ë§‰ëŒ€ê·¸ëž˜í”„ë¥¼ ê·¸ë¦¬ì„¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y):\n",
    "    X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=32)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_decision_tree(X_train, y_train):\n",
    "    model = DeicisionTreeClassifier(random_state=32)\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "def plot_featrue_importances(model, feature_names):\n",
    "    plt.figure(dpi=150)\n",
    "    importances = model.feature_importances_\n",
    "    plt.barh(feature_names, importances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a7806",
   "metadata": {},
   "source": [
    "## ðŸ“Œ í´ëŸ¬ìŠ¤í„°ë§ ì˜ˆìƒ ë¬¸ì œ 13\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **`k_means_clus()` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.**\n",
    "    - `KMeans` ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ, `init` íŒŒë¼ë¯¸í„°ë¥¼ â€˜randomâ€™ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "    - ìƒì„±ëœ `kmeans` ê°ì²´ë¥¼ `spamDF`ì—ì„œ **'ìŠ¤íŒ¸ì—¬ë¶€' ì»¬ëŸ¼ì„ ì œì™¸í•œ** ëª¨ë“  íŠ¹ì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¤ì„¸ìš”.\n",
    "\n",
    "1. **`Visualize()` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.**\n",
    "    - `PCA` ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ, ë°ì´í„°ë¥¼ **2ê°œì˜ ì£¼ì„±ë¶„**(`n_components`)ìœ¼ë¡œ ì¶•ì†Œí•˜ë„ë¡ ì„¤ì •í•˜ì„¸ìš”.\n",
    "    - êµ°ì§‘í™” í•  ê°œìˆ˜ `n_clusters` ëŠ” **2**ë¡œ ì§€ì •í•´ì£¼ê³ , `random_state`ëŠ” **123**ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    - ìƒì„±ëœ `pca` ê°ì²´ë¥¼ `spamDF`ì—ì„œ **'ìŠ¤íŒ¸ì—¬ë¶€' ì»¬ëŸ¼ì„ ì œì™¸í•œ** ëª¨ë“  íŠ¹ì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¤ì„¸ìš”.\n",
    "\n",
    "1. í•™ìŠµëœ `pca` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ `spamDF`ì—ì„œ **'ìŠ¤íŒ¸ì—¬ë¶€' ì»¬ëŸ¼ì„ ì œì™¸í•œ** ëª¨ë“  íŠ¹ì„± ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ **`pca_transformed` ë³€ìˆ˜**ì— ì €ìž¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def k_means_clus(spamDF):\n",
    "    kmeans = KMeans(init='random', n_clusters=2, random_state=123)\n",
    "    kmeans.fit(spamDF.drop(columns=['ìŠ¤íŒ¸ì—¬ë¶€'], axis=1))\n",
    "\n",
    "    spamDF['cluster'] = kmeans.labels_\n",
    "    spam_result = spamDF.groupby(['ìŠ¤íŒ¸ì—¬ë¶€','í´ëŸ¬ìŠ¤í„°'])['ë¬´ë£Œ_ë‹¨ì–´_ë¹ˆë„'].count()\n",
    "\n",
    "def Visulaize(spamDF):\n",
    "    pca = PCA(n_components=2, random_state=123)\n",
    "    pca.fit(spamDF.drop(columns=['ìŠ¤íŒ¸ì—¬ë¶€'], axis=1))\n",
    "\n",
    "    pca_transformed = pca.transform(spamDF.drop(columns=['ìŠ¤íŒ¸ì—¬ë¶€'], axis=1))\n",
    "\n",
    "    spamDF['pca_x'] = pca_transformed[:,0]\n",
    "    spamDF['pca_y'] = pca_transformed[:,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf8fb5",
   "metadata": {},
   "source": [
    "## ðŸ“Œ KNN ì˜ˆìƒ ë¬¸ì œ 14\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **`ì„±ë³„` ì»¬ëŸ¼ì„ ìˆ«ìžë¡œ ì¸ì½”ë”©**í•˜ì„¸ìš”.\n",
    "    - `'ë‚¨'`ì€ 0, `'ì—¬'`ëŠ” 1ë¡œ ë§¤í•‘í•˜ì—¬ ë°˜í™˜í•˜ëŠ” `encode_labels()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "\n",
    "1. **ìž…ë ¥ ë³€ìˆ˜(íŠ¹ì§•) ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  í‘œì¤€í™”**í•˜ì„¸ìš”.\n",
    "    - `id`, `ì„±ë³„` ì»¬ëŸ¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì—¬ í‘œì¤€í™”(í‰ê·  0, ë¶„ì‚° 1)í•˜ëŠ” `extract_and_scale_features()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "\n",
    "1. ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ **7:3 ë¹„ìœ¨**ë¡œ ë¶„í• í•˜ì„¸ìš”.\n",
    "    - **`random_state=32`**, **`stratify=y`** ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ `split_data()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "\n",
    "1. **K-ìµœê·¼ì ‘ ì´ì›ƒ ë¶„ë¥˜ê¸°(KNeighborsClassifier)** ë¥¼ ì‚¬ìš©í•´ í•™ìŠµ ë° ì˜ˆì¸¡í•˜ì„¸ìš”.\n",
    "    - ì´ì›ƒì˜ ê°œìˆ˜(**`n_neighbors`**)ëŠ” **10**ìœ¼ë¡œ ì„¤ì •í•˜ê³ , **`train_and_predict()`** í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "    - í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì •í™•ë„(**accuracy**)ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def encode_labels(df):\n",
    "    return df['ì„±ë³„'].map({'ë‚¨':0, 'ì—¬':1})\n",
    "\n",
    "def extract_and_scale_features(df):\n",
    "    X = df.drop(columns=['id','ì„±ë³„'], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "def split_data(X,y):\n",
    "    return train_test_split(X,y,test_size=0.3, stratify=y, random_state=32)\n",
    "\n",
    "def train_and_predict(X_train, X_test, y_train, y_test):\n",
    "    model = KNeighborsClassifier(n_neighbors=10)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    return model, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be53ccb",
   "metadata": {},
   "source": [
    "## ðŸ“Œ ë”¥ëŸ¬ë‹ êµ¬í˜„ ì˜ˆìƒ ë¬¸ì œ 15\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "| **í•¨ìˆ˜ ì´ë¦„** | **ì¸µ ì¢…ë¥˜** | **ìœ ë‹› ìˆ˜** | **í™œì„±í™” í•¨ìˆ˜** | **ì„¤ëª…** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| `make_model1()` | ìž…ë ¥ì¸µ | 2 | ì—†ìŒ | ìž…ë ¥: íŠ¹ì§• 2ê°œ |\n",
    "|  | ì€ë‹‰ì¸µ 1 | 120 | ì—†ìŒ | ê¸°ë³¸ ì„ í˜• |\n",
    "|  | ì€ë‹‰ì¸µ 2 | 250 | ReLU | ë¹„ì„ í˜• í•™ìŠµ ê°€ëŠ¥ |\n",
    "|  | ì€ë‹‰ì¸µ 3 | 60 | ReLU | ì¶”ê°€ ë¹„ì„ í˜• í‘œí˜„ |\n",
    "|  | ì¶œë ¥ì¸µ | 12 | Softmax | 12ê°œ í´ëž˜ìŠ¤ ë¶„ë¥˜ |\n",
    "| `make_model2()` | ìž…ë ¥ì¸µ | 100 | ì—†ìŒ | ìž…ë ¥: íŠ¹ì§• 100ê°œ |\n",
    "|  | ì€ë‹‰ì¸µ 1 | 30 | ì—†ìŒ | ê¸°ë³¸ ì„ í˜• |\n",
    "|  | ì€ë‹‰ì¸µ 2 | 140 | ReLU | íŠ¹ì„± ì¶”ì¶œ |\n",
    "|  | ì¶œë ¥ì¸µ | 4 | Softmax | 4ê°œ í´ëž˜ìŠ¤ ë¶„ë¥˜ |\n",
    "| `make_model3()` | ìž…ë ¥ì¸µ | 10 | ì—†ìŒ | ìž…ë ¥: íŠ¹ì§• 10ê°œ |\n",
    "|  | ì€ë‹‰ì¸µ 1 | 110 | ì—†ìŒ | ê¸°ë³¸ ì„ í˜• |\n",
    "|  | ì€ë‹‰ì¸µ 2 | 200 | Sigmoid | ë³µìž¡í•œ ê´€ê³„ ëª¨ë¸ë§ ê°€ëŠ¥ |\n",
    "|  | ì€ë‹‰ì¸µ 3 | 40 | ReLU | ë¹„ì„ í˜• í‘œí˜„ ì¶”ê°€ |\n",
    "|  | ì¶œë ¥ì¸µ | 1 | ì—†ìŒ | íšŒê·€ ë¬¸ì œ (ìˆ«ìž ì˜ˆì¸¡) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def make_model1():\n",
    "    model1 = keras.models.Sequential([\n",
    "        keras.layers.Dense(units=120, input_dim=2), # input_shape=(2,)\n",
    "        keras.layers.Dense(units=250, activation='relu'),\n",
    "        keras.layers.Dense(units=60, activation='relu'),\n",
    "        keras.layers.Dense(units=12, activation='softmax'),\n",
    "    ])\n",
    "    return model1\n",
    "\n",
    "def make_model2():\n",
    "    model2 = keras.models.Sequential([\n",
    "        keras.layers.Dense(units=30, input_shape=(100,)),\n",
    "        keras.layers.Dense(units=140, activation='relu'),\n",
    "        keras.layers.Dense(units=4, activation='softmax')\n",
    "    ])\n",
    "    return model2\n",
    "\n",
    "def make_model3():\n",
    "    model3 = keras.models.Sequential([\n",
    "        keras.layers.Dense(units=110, input_shape=(10,)),\n",
    "        keras.layers.Dense(units=200, activation='sigmoid'),\n",
    "        keras.layers.Dense(units=40, activation='relu'),\n",
    "        keras.layers.Dense(units=1)\n",
    "    ])\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996c5fa",
   "metadata": {},
   "source": [
    "## ðŸ“Œ ì›í•«ì¸ì½”ë”© ì˜ˆìƒ ë¬¸ì œ 16\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **sklearn**ì˜ **preprocessing** ëª¨ë“ˆì—ì„œ **StandardScaler, OneHotEncoder**ë¥¼ `import`í•˜ì„¸ìš”.\n",
    "\n",
    "1. **ë…ë¦½ë³€ìˆ˜ X** ì™€ **ì¢…ì†ë³€ìˆ˜ y**ë¥¼ ìž…ë ¥ë°›ì•„ í•™ìŠµì— ìš©ì´í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    1. XëŠ” **StandardScaler**ë¥¼ ì‚¬ìš©í•´ **í‘œì¤€í™”**í•©ë‹ˆë‹¤.\n",
    "    2. yëŠ” **OneHotEncoder**ë¥¼ ì‚¬ìš©í•´ **ì¸ì½”ë”©**í•©ë‹ˆë‹¤. **sparse = False** ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n",
    "    3. train_test_split ì‚¬ìš©í•´ì„œ Xì™€ yë¥¼ **í•™ìŠµìš©, í…ŒìŠ¤íŠ¸ìš©**ìœ¼ë¡œ `test_size = 0.2` ë¡œ ë¶„í• í•˜ê³ , `random_state`ë¥¼ **42**ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "\n",
    "1. **keras**ë¥¼ ì‚¬ìš©í•´ í•™ìŠµì‹œí‚¬ ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "    1. **ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ**ì€ **ìœ ë‹› = 64**, **í™œì„±í™” í•¨ìˆ˜**ëŠ” **â€œreluâ€**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìž…ë ¥ ì°¨ì›ì€ **íŠœí”Œ (4, )**ë¡œ ì§€ì •í•˜ì„¸ìš”.\n",
    "    2. **ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ**ì€ **ìœ ë‹› = 32**, **í™œì„±í™” í•¨ìˆ˜**ëŠ” **â€œreluâ€**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    3. **ì¶œë ¥ì¸µ**ì€ **ìœ ë‹› = 5**, **í™œì„±í™” í•¨ìˆ˜**ë¡œ **â€œsoftmaxâ€**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "\n",
    "1. modelì„ í•™ìŠµì‹œí‚¤ì„¸ìš”.\n",
    "    1. ì»´íŒŒì¼ ì‹œ **Optimizer** ëŠ” **â€œadamâ€**, **loss**ëŠ” **â€œcategorical_crossentropyâ€**, **metrics**ëŠ” **â€œaccuracyâ€**ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "    2. **X_train**ê³¼ **y_train**ì„ í•™ìŠµí•˜ê³  **epochs**ëŠ” **5**ë¡œ ì§€ì •í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def ready_data():\n",
    "    df = pd.read_csv('dog_data.csv')\n",
    "    df = df.dropna()\n",
    "    X = df[['height', 'weight', 'ear_length', 'tail']].values\n",
    "    y = df['breed'].values.reshape(-1,1)\n",
    "    return X,y\n",
    "\n",
    "def data_transform(X,y):\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    y = encoder.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_State=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(units=64, activation = 'relu', input_shape=(4,)),\n",
    "        keras.layers.Dense(units=32, activation = 'relu'),\n",
    "        kears.layers.Dense(units=5, activation = 'softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = 'adam', \n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics =['acuuracy']\n",
    "    )\n",
    "    model.fit(X_train,y_train, epochs=5)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4d55d",
   "metadata": {},
   "source": [
    "## ðŸ“Œ ì›í•«ì¸ì½”ë”© ì˜ˆìƒ ë¬¸ì œ 17\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. dfì˜ â€œ**ì¶œê²°ìƒí™©**â€ ì»¬ëŸ¼ëª…ì„ â€œ**ì¶œê²°**â€ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "1. â€œ**ê¸°ë§ê³ ì‚¬**â€ ì»¬ëŸ¼ì„ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ„ì–´ **0, 1, 2ì˜ ê°’**ì„ ê°–ëŠ” **ë²”ì£¼í˜• ë°ì´í„°**ë¡œ **ë³€í™˜**í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    1. 0: ë‚®ì€ ì ìˆ˜ (0 â‰¤ score < 60)\n",
    "    2. 1 : ì¤‘ê°„ ì ìˆ˜ (60 â‰¤ score < 80)\n",
    "    3. 2 : ë†’ì€ ì ìˆ˜ (80 â‰¤ score â‰¤ 100)\n",
    "    4. â€œ**ê¸°ë§ê³ ì‚¬**â€ ì»¬ëŸ¼ì„ **ë¹„ë‹í•˜ì—¬ ë°˜í™˜**í•˜ì„¸ìš”.\n",
    "\n",
    "1. â€œ**ê¸°ë§ê³ ì‚¬**â€ì»¬ëŸ¼ì„ ëŒ€ìƒìœ¼ë¡œ **ì›í•«ì¸ì½”ë”©**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    1. **pandas**ì˜ **get_dummies()** í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_name(df):\n",
    "    df = df.rename(columns={'ì¶œê²°ìƒí™©':'ì¶œê²°'})\n",
    "    return df\n",
    "\n",
    "def bin_final_score(df):\n",
    "    def binning(score):\n",
    "        if pd.isnull(score): #nullì´ë©´ nullë¡œ ë°˜í™˜\n",
    "            return score\n",
    "        elif score <60:\n",
    "            return 0\n",
    "        elif 60<=score<80:\n",
    "            return 1\n",
    "        elif 80<=score<=100:\n",
    "            return 2\n",
    "    df['ê¸°ë§ê³ ì‚¬'] = df['ê¸°ë§ê³ ì‚¬'].map(binning)\n",
    "    return df\n",
    "\n",
    "def encode_grade(df):\n",
    "    df = pd.get_dummies(df, columns=['ê¸°ë§ê³ ì‚¬'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5aef06",
   "metadata": {},
   "source": [
    "## ðŸ“ŒÂ CNN, RNN ì˜ˆìƒ ë¬¸ì œ 18\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­\n",
    "\n",
    "1. **Sequential** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **CNN ëª¨ë¸**ì„ ì •ì˜í•˜ëŠ” **cnn()** í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    - **ìž…ë ¥: (32, 32, 1)**\n",
    "    - ì²« ë²ˆì§¸ **Conv2D** ì¸µ\n",
    "        \n",
    "        â†’ filters **: 8**, **ì»¤ë„ í¬ê¸° (5, 5)**, **í™œì„±í™” í•¨ìˆ˜ 'relu'**\n",
    "        \n",
    "    - ë‘ ë²ˆì§¸ **Conv2D** ì¸µ\n",
    "        \n",
    "        â†’ filters **: 16, ì»¤ë„ í¬ê¸° (3, 3), í™œì„±í™” í•¨ìˆ˜ 'relu'**\n",
    "        \n",
    "    - ì´í›„ **Flatten()** â†’ **Dense(10, softmax)** ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "1. **Sequential** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **RNN ëª¨ë¸**ì„ ì •ì˜í•˜ëŠ” **rnn()** í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "    - **ìž…ë ¥: (30, 2)**\n",
    "    - **SimpleRNN** ì¸µ â†’ **ìœ ë‹› ìˆ˜: 64**, **í™œì„±í™” í•¨ìˆ˜ 'relu'**\n",
    "    - ì´í›„ **Dense(5, softmax)** ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, SimpleRNN\n",
    "\n",
    "def cnn():\n",
    "    cnn = Sequential([\n",
    "        Input(shape=(32,32,1))\n",
    "        Conv2D(8, kernel_size=(5,5), activation='relu'),\n",
    "        Conv2D(16, kernel_size=(3,3), activation='relu'),\n",
    "        Flatten()\n",
    "        Dense(10, activation ='softmax')\n",
    "    ])\n",
    "    return cnn\n",
    "\n",
    "def rnn():\n",
    "    rnn =Sequential([\n",
    "        Input(shape=(30,2)),\n",
    "        SimpleRNN(units=64, activation='relu'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return rnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
